<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Natural Language Processing - Tag - Corentin LEGER</title>
        <link>http://example.org/tags/natural-language-processing/</link>
        <description>Natural Language Processing - Tag - Corentin LEGER</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 05 Jul 2024 11:00:59 -0400</lastBuildDate><atom:link href="http://example.org/tags/natural-language-processing/" rel="self" type="application/rss+xml" /><item>
    <title>Research: When LLMs Play the Telephone Game</title>
    <link>http://example.org/posts/telephonellm/</link>
    <pubDate>Fri, 05 Jul 2024 11:00:59 -0400</pubDate>
    <author>Jérémy Perez, Corentin Léger, Grgur Kovač, Cédric Colas, Gaia Molinaro, Maxime Derex, Pierre-Yves Oudeyer, Clément Moulin-Frier</author>
    <guid>http://example.org/posts/telephonellm/</guid>
    <description><![CDATA[Arxiv preprint (Under Review)
Abstract As large language models (LLMs) start interacting with each other and generating an increasing amount of text online, it becomes crucial to better understand how information is transformed as it passes from one LLM to the next. While significant research has examined individual LLM behaviors, existing studies have largely overlooked the collective behaviors and information distortions arising from iterated LLM interactions. Small biases, negligible at the single output level, risk being amplified in iterated interactions, potentially leading the content to evolve towards attractor states.]]></description>
</item>
</channel>
</rss>
