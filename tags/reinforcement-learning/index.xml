<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement learning on Corentin LEGER</title><link>https://corentinlger.github.io/tags/reinforcement-learning/</link><description>Recent content in Reinforcement learning on Corentin LEGER</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 10 Sep 2022 11:00:59 -0400</lastBuildDate><atom:link href="https://corentinlger.github.io/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Symbolic Reinforcement learning - Research project with Inria</title><link>https://corentinlger.github.io/post/symbolicrl/</link><pubDate>Sat, 10 Sep 2022 11:00:59 -0400</pubDate><guid>https://corentinlger.github.io/post/symbolicrl/</guid><description>Modeling complex problems is a real issue in Reinforcement learning. The goal of this projet is to use a RL model, such as Q-learning, on symbolic data, where the knowledge of the problem would be stored, in the form of qualitative or numerical data (in JSON for example). Instead of only working with numerical data, we could link the data with a certain &amp;rsquo;edition distance&amp;rsquo; which permits to evaluate the proximity of 2 elements.</description></item></channel></rss>